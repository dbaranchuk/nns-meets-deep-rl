{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity graph construction experiment on SIFT100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import os.path as osp\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "sys.path.append('..')\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import lib\n",
    "\n",
    "print(\"Numpy: {}, Torch: {}\".format(np.__version__, torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download vertices, graph edges and ground truth neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/SIFT100K'\n",
    "\n",
    "if not osp.exists(DATA_DIR):\n",
    "    assert not DATA_DIR.endswith(os.sep), 'please do not put \"/\" at the end of DATA_DIR'\n",
    "    !mkdir -p {DATA_DIR}\n",
    "    !wget https://www.dropbox.com/sh/vw6ojdvldg3ph1k/AABp2J2_r7FajjDvnqTgGKvSa?dl=1 -O {DATA_DIR}/sift_100k.zip\n",
    "    !cd {DATA_DIR} && unzip sift_100k.zip && rm sift_100k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "NOTE: this config requires ~10.5GB GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Graph params #\n",
    "################\n",
    "\n",
    "graph_type = 'nsw'        # 'nsw' or 'nsg'\n",
    "M = 12                    # degree parameter for NSW (Max degree is 2*M)\n",
    "ef = 5                    # search algorithm parameter, sets the operating point\n",
    "R = 16                    # degree parameter for NSG (corresponds to max degree)\n",
    "k = 1                     # Number of answers per query. Need for Recall@k\n",
    "\n",
    "assert k <= ef\n",
    "\n",
    "nn = 200                  # Number of NN in initial KNNG that is used for NSG construction \n",
    "efC = 300                 # efConstruction used for NSW graph\n",
    "ngt = 100                 # Number of ground truth answers per query\n",
    "val_queries_size = 20000  # Number of queries for validation\n",
    "\n",
    "#################\n",
    "# Reward params #\n",
    "#################\n",
    "\n",
    "max_dcs = 800             # reward hyperparameter\n",
    "\n",
    "################\n",
    "# Agent params #\n",
    "################\n",
    "\n",
    "hidden_size = 2048        # number of hidden units\n",
    "\n",
    "####################\n",
    "# Algorithm params #\n",
    "####################\n",
    "\n",
    "samples_in_batch = 90000  # Reduce for larger hidden_size to fit in GPU memory \n",
    "Fvp_speedup = 5           # fraction of samples for Fisher vector product estimation \n",
    "                          # Reflects on the iteration time (<10 is okay)\n",
    "Fvp_type = 'fim'          # Fisher vector product implementation: ['forward', 'fim']\n",
    "entropy_reg = 0.01        # coefficient in front of the entropy regularizer term \n",
    "batch_size = 50000        # number of sessions per batch\n",
    "\n",
    "n_jobs = 8                # Number of threads for C++ sampling\n",
    "max_steps = 1000          # Max number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib\n",
    "\n",
    "graph_params = { \n",
    "    'vertices_path': osp.join(DATA_DIR, 'sift_base.fvecs'),\n",
    "\n",
    "    'train_queries_path': osp.join(DATA_DIR, 'sift_learn.fvecs'),\n",
    "    'test_queries_path': osp.join(DATA_DIR, 'sift_query.fvecs'),\n",
    "    \n",
    "    'train_gt_path': osp.join(DATA_DIR, 'train_gt.ivecs'),\n",
    "    'test_gt_path': osp.join(DATA_DIR, 'test_gt.ivecs'),\n",
    "#     ^-- comment these 2 lines to re-compute ground truth ids (if you don't have pre-computed ground truths)\n",
    "    \n",
    "    'val_queries_size': val_queries_size,\n",
    "    'ground_truth_n_neighbors': ngt,  # for each query, finds this many nearest neighbors via brute force\n",
    "    'graph_type': graph_type\n",
    "}\n",
    "\n",
    "if graph_type == 'nsg':\n",
    "    graph_params['edges_path'] = osp.join(DATA_DIR, 'sift_R{R}_{nn}nn.nsg'.format(R=R, nn=nn))\n",
    "elif graph_type == 'nsw':\n",
    "    graph_params['edges_path'] = osp.join(DATA_DIR, 'sift_nsw_M{M}_efC{efC}.ivecs'.format(M=M, efC=efC))\n",
    "    graph_params['initial_vertex_id'] = 0 # by default, starts search from this vertex\n",
    "else:\n",
    "    raise ValueError(\"Wrong graph type: ['nsg', 'nsw']\")\n",
    "    \n",
    "graph = lib.Graph(**graph_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if graph_type == 'nsw':\n",
    "    exp_name = '{data_name}_{graph_type}_k{k}_M{M}_ef{ef}_max-dcs{max_dcs}_hid-size{hidden_size}_entropy{entropy_reg}'.format(\n",
    "        data_name=osp.split(DATA_DIR)[-1], k=k, M=M, ef=ef, hidden_size=hidden_size, \n",
    "        max_dcs=max_dcs, entropy_reg=entropy_reg, graph_type=graph_type\n",
    "    )\n",
    "elif graph_type == 'nsg':\n",
    "    exp_name = '{data_name}_{graph_type}_k{k}_R{R}_ef{ef}_max-dcs{max_dcs}_hid-size{hidden_size}_entropy{entropy_reg}'.format(\n",
    "        data_name=osp.split(DATA_DIR)[-1], k=k, R=R, ef=ef, hidden_size=hidden_size, \n",
    "        max_dcs=max_dcs, entropy_reg=entropy_reg, graph_type=graph_type\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Wrong graph type: ['nsg', 'nsw']\")\n",
    "    \n",
    "print('exp name:', exp_name)\n",
    "#!rm {'./runs/' + exp_name} -rf # KEEP COMMENTED!\n",
    "assert not os.path.exists('./runs/' + exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HNSW, Agent, Reward, Baseline and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnsw = lib.ParallelHNSW(graph, ef=ef, k=k, n_jobs=n_jobs)\n",
    "agent = lib.SimpleNeuralAgent(graph.vertices.shape[1], hidden_size=hidden_size)\n",
    "reward = lib.MaxDCSReward(k=k, max_dcs=max_dcs)\n",
    "baseline = lib.SessionBaseline(graph.train_queries.size(0))\n",
    "trainer = lib.EfficientTRPO(agent, hnsw, reward, baseline,\n",
    "                            samples_in_batch=samples_in_batch,\n",
    "                            Fvp_type=Fvp_type,\n",
    "                            Fvp_speedup=Fvp_speedup, entropy_reg=entropy_reg,\n",
    "                            writer=SummaryWriter('./runs/' + exp_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "moving_average = lambda x, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "reward_history = []\n",
    "\n",
    "# generate batches of [queries, ground truth, train_query_ids (for baseline)]\n",
    "train_query_ids = torch.arange(graph.train_queries.size(0))\n",
    "train_batcher = lib.utils.iterate_minibatches(graph.train_queries, graph.train_gt, train_query_ids, \n",
    "                                              batch_size=batch_size)\n",
    "\n",
    "# generate batches of [queries, ground truth]           \n",
    "val_iterator = lib.utils.iterate_minibatches(graph.val_queries, graph.val_gt, \n",
    "                                             batch_size=graph.val_queries.size(0))\n",
    "\n",
    "dev_iterator = lib.utils.iterate_minibatches(graph.test_queries, graph.test_gt, \n",
    "                                             batch_size=graph.test_queries.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch_queries, batch_gt, batch_query_ids in train_batcher:\n",
    "    start = time.time()\n",
    "    torch.cuda.empty_cache()\n",
    "    mean_reward = trainer.train_step(batch_queries, batch_gt, query_index=batch_query_ids)\n",
    "    reward_history.append(mean_reward)\n",
    "        \n",
    "    if trainer.step % 10 == 0:\n",
    "        trainer.evaluate(*next(val_iterator), prefix='val')\n",
    "        \n",
    "    if trainer.step % 50 == 0:\n",
    "        trainer.evaluate(*next(dev_iterator))\n",
    "        print(end=\"Saving...\")\n",
    "        torch.save(agent, \"runs/{}/agent.{}.pth\".format(exp_name, trainer.step))\n",
    "        torch.save(baseline, \"runs/{}/baseline.{}.pth\".format(exp_name, trainer.step))\n",
    "        print('Done!')\n",
    "    \n",
    "    if trainer.step % 1 == 0:\n",
    "        clear_output(True)\n",
    "        plt.title('train reward over time')\n",
    "        plt.plot(moving_average(reward_history, span=50))\n",
    "        plt.scatter(range(len(reward_history)), reward_history, alpha=0.1)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        print(\"step=%i, mean_reward=%.3f, time=%.3f\" % \n",
    "              (trainer.step, np.mean(reward_history[-100:]), time.time()-start))\n",
    "    \n",
    "    if trainer.step >= max_steps: break\n",
    "\n",
    "#protip: run tensorboard in ./runs to get all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_step = None\n",
    "\n",
    "if best_val_step is not None:\n",
    "    agent = torch.load(\"runs/{}/agent.{}.pth\".format(exp_name, best_val_step))\n",
    "    trainer.step = best_val_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "agent.cuda()\n",
    "state = agent.prepare_state(graph, device='cuda')\n",
    "\n",
    "new_edges = defaultdict(list)\n",
    "for vector_id in graph.edges.keys():\n",
    "    list_edges = list(graph.edges[vector_id])\n",
    "    d = len(list_edges)\n",
    "    with torch.no_grad():\n",
    "        edges_logp = agent.get_edge_logp([vector_id]*d, list_edges, state=state, device='cuda').cpu()\n",
    "        edges_mask = edges_logp.argmax(-1).numpy() == 1\n",
    "    new_edges[vector_id] = [edge for i, edge in enumerate(list_edges) if edges_mask[i] == 1]\n",
    "\n",
    "#Save constructed graph\n",
    "lib.write_edges(\"runs/{}/graph.{}.ivecs\".format(exp_name, trainer.step), new_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate constructed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for heap_size in [1, 2, 3, 5, 10, 20, 30]:\n",
    "    algo_hnsw = lib.BaseAlgorithm(\n",
    "        agent=agent,\n",
    "        hnsw=lib.ParallelHNSW(graph, ef=heap_size, k=k, n_jobs=n_jobs),\n",
    "        reward=lambda actions, **kw: [0] * len(actions),\n",
    "        writer=trainer.writer, device='cuda',\n",
    "    )\n",
    "    algo_hnsw.step = trainer.step  # for tensorboard\n",
    "\n",
    "    metrics = algo_hnsw.get_session_batch(graph.test_queries, graph.test_gt, greedy=True,\n",
    "                             summarize=True, write_logs=False, prefix='dev',)['summary']\n",
    "    sys.stderr.flush()\n",
    "    print(\"Ef %i | Recall@%d %f | Distances: %f\" % \n",
    "          (heap_size, k, metrics['dev/recall@%d' % k], metrics['dev/distance_computations']),\n",
    "          flush=True,\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
